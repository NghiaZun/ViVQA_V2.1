"""
LATENT REASONING VQA - FIXED VERSION
=====================================

CRITICAL FIXES for 9 deadly issues:

1. âœ… BOTTLENECK ENFORCEMENT - Reasoning-only conditioning
2. âœ… POSTERIOR COLLAPSE FIX - KL warmup + free bits + stop gradient
3. âœ… VISION GROUNDING - Vision-first fusion + image dropout
4. âœ… PROPER LATENT SIZE - 4-8 tokens Ã— 256 dim (not 16Ã—1024!)
5. âœ… DIVERSITY ENFORCEMENT - Orthogonality + token dropout
6. âœ… CAUSAL INTERVENTION - Reasoning ablation built-in
7. âœ… DATASET FILTERING - Hard examples only
8. âœ… TRAINING CURRICULUM - Simple to complex
9. âœ… REASONING METRICS - Intervention tests

This is the CORRECT implementation.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass
from typing import Optional, Tuple

from transformers import (
    AutoModel,
    AutoImageProcessor,
    BartphoTokenizer,
    MBartForConditionalGeneration
)


def shift_tokens_right(input_ids, pad_token_id, decoder_start_token_id):
    """Shift tokens right for teacher forcing"""
    shifted_input_ids = input_ids.new_zeros(input_ids.shape)
    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()
    shifted_input_ids[:, 0] = decoder_start_token_id
    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)
    return shifted_input_ids


# ============================================================================
# FIX #3: VISION-FIRST FUSION (prevent text shortcut)
# ============================================================================

class VisionFirstFusion(nn.Module):
    """
    Vision-first cross-attention to prevent text shortcuts
    
    Key idea: Force model to attend to vision BEFORE using text
    """
    def __init__(self, hidden_dim=1024, num_heads=16, dropout=0.1):
        super().__init__()
        
        # Vision â†’ Text attention (vision queries text)
        self.vision_to_text = nn.MultiheadAttention(
            hidden_dim, num_heads, dropout=dropout, batch_first=True
        )
        
        # Text â†’ Enhanced Vision attention
        self.text_to_vision = nn.MultiheadAttention(
            hidden_dim, num_heads, dropout=dropout, batch_first=True
        )
        
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        
        # Gating
        self.gate = nn.Linear(hidden_dim * 2, hidden_dim)
        
    def forward(self, text_features, visual_features, image_dropout_prob=0.0):
        """
        Args:
            image_dropout_prob: FIX #3 - randomly drop images during training
        """
        # FIX #3: Image dropout to force robustness
        if self.training and image_dropout_prob > 0:
            batch_size = visual_features.size(0)
            keep_mask = torch.rand(batch_size, 1, 1, device=visual_features.device) > image_dropout_prob
            visual_features = visual_features * keep_mask
        
        # Step 1: Vision queries text (vision-grounded text)
        vision_grounded, _ = self.vision_to_text(
            query=visual_features, key=text_features, value=text_features
        )
        vision_enhanced = self.norm1(visual_features + vision_grounded)
        
        # Step 2: Text attends to enhanced vision
        text_enhanced, attn = self.text_to_vision(
            query=text_features, key=vision_enhanced, value=vision_enhanced
        )
        
        # Gating
        gate_input = torch.cat([text_features, text_enhanced], dim=-1)
        gate = torch.sigmoid(self.gate(gate_input))
        
        fused = gate * text_enhanced + (1 - gate) * text_features
        fused = self.norm2(fused)
        
        return fused, attn


# ============================================================================
# FIX #4: PROPER LATENT DIMENSIONALITY
# ============================================================================

class CompressedLatentReasoning(nn.Module):
    """
    FIX #4: Small latent bottleneck
    
    - Only 4-8 tokens (not 16!)
    - Only 256 dim (not 1024!)
    - True information bottleneck
    """
    def __init__(
        self,
        input_dim: int = 1024,
        num_tokens: int = 4,  # FIX #4: Much smaller!
        latent_dim: int = 256,  # FIX #4: Compressed!
        num_heads: int = 8,
        num_layers: int = 2,
        dropout: float = 0.1,
        free_bits: float = 0.5,  # FIX #2: Prevent collapse
    ):
        super().__init__()
        self.num_tokens = num_tokens
        self.latent_dim = latent_dim
        self.free_bits = free_bits
        
        # Learnable queries (small!)
        self.reasoning_queries = nn.Parameter(
            torch.randn(num_tokens, input_dim) * 0.02
        )
        
        # Cross-attention to extract reasoning
        self.cross_attn_layers = nn.ModuleList([
            nn.TransformerDecoderLayer(
                d_model=input_dim, nhead=num_heads,
                dim_feedforward=input_dim * 2,  # Smaller FFN
                dropout=dropout, activation='gelu',
                batch_first=True, norm_first=True
            )
            for _ in range(num_layers)
        ])
        
        # FIX #4: Compress to small latent
        self.to_latent = nn.Sequential(
            nn.Linear(input_dim, latent_dim),
            nn.LayerNorm(latent_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # VAE components
        self.to_mu = nn.Linear(latent_dim, latent_dim)
        self.to_logvar = nn.Linear(latent_dim, latent_dim)
        
        # FIX #1: Map back to input_dim for decoder (not latent_dim!)
        self.from_latent = nn.Sequential(
            nn.Linear(latent_dim, input_dim),
            nn.LayerNorm(input_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def compute_kl_with_free_bits(self, mu, logvar):
        """
        FIX #2: Free bits to prevent posterior collapse
        
        Computes KL divergence: KL(q(z|x) || p(z)) where p(z) = N(0, I)
        Formula: -0.5 * sum(1 + log(var) - mu^2 - var)
        
        Shape:
            mu, logvar: [batch_size, num_tokens, latent_dim]
            output: scalar (mean over batch)
        """
        # Standard KL per dimension: -0.5 * (1 + logvar - mu^2 - exp(logvar))
        # Use MEAN over latent_dim (not SUM) to avoid scaling by dimension size
        kl_per_token = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp(), dim=-1)
        # kl_per_token shape: [batch_size, num_tokens]
        
        # Free bits: only penalize if KL > free_bits (per token)
        # With MEAN computation, typical KL ~ 0.1-0.5, so free_bits should be small
        # Set to 0.0 to disable (warmup schedule handles collapse prevention)
        if self.free_bits > 0:
            kl_per_token = torch.clamp(kl_per_token - self.free_bits, min=0.0)
        
        # Average over tokens and batch
        return kl_per_token.mean()
    
    def forward(
        self, 
        multimodal_features, 
        attention_mask=None, 
        deterministic=False,
        stop_gradient=False,  # FIX #2: Stop gradient from decoder
        temperature=1.0  # PROPOSAL: Temperature for stochastic sampling
    ):
        batch_size = multimodal_features.size(0)
        
        # Expand queries
        queries = self.reasoning_queries.unsqueeze(0).expand(batch_size, -1, -1)
        
        # Cross-attend
        for layer in self.cross_attn_layers:
            queries = layer(
                tgt=queries, memory=multimodal_features,
                memory_key_padding_mask=~attention_mask.bool() if attention_mask is not None else None
            )
        
        # FIX #2: Stop gradient if requested
        if stop_gradient and self.training:
            queries = queries.detach()
        
        # Compress to latent
        compressed = self.to_latent(queries)  # [B, num_tokens, latent_dim]
        
        # VAE sampling
        mu = self.to_mu(compressed)
        logvar = self.to_logvar(compressed)
        
        if deterministic or not self.training:
            z = mu
        else:
            # PROPOSAL: Stochastic sampling with temperature
            # Higher temperature = more exploration of reasoning space
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            z = mu + temperature * std * eps  # Temperature-scaled sampling
        
        # FIX #2: KL with free bits
        kl_loss = self.compute_kl_with_free_bits(mu, logvar)
        
        # Expand back to input_dim
        reasoning_output = self.from_latent(z)
        
        return reasoning_output, kl_loss, z, mu, logvar


# ============================================================================
# FIX #5: DIVERSITY ENFORCEMENT
# ============================================================================

class DiversityRegularizer:
    """
    FIX #5: Prevent token collapse
    
    - Orthogonality loss
    - Token-wise dropout
    - Diversity monitoring
    """
    def __init__(
        self,
        ortho_weight: float = 0.1,
        token_dropout_prob: float = 0.3,
        min_std_threshold: float = 0.01
    ):
        self.ortho_weight = ortho_weight
        self.token_dropout_prob = token_dropout_prob
        self.min_std_threshold = min_std_threshold
    
    def compute_orthogonality_loss(self, tokens):
        """
        Force tokens to be orthogonal (diverse)
        """
        # tokens: [B, num_tokens, dim]
        normalized = F.normalize(tokens, p=2, dim=-1)
        
        # Gram matrix: [B, num_tokens, num_tokens]
        gram = torch.bmm(normalized, normalized.transpose(1, 2))
        
        # Want identity matrix
        batch_size, num_tokens, _ = gram.shape
        identity = torch.eye(num_tokens, device=tokens.device).unsqueeze(0).expand(batch_size, -1, -1)
        
        # Frobenius norm of difference
        ortho_loss = F.mse_loss(gram, identity)
        
        return ortho_loss
    
    def apply_token_dropout(self, tokens, training=True):
        """
        FIX #5: Token dropout for robustness
        """
        if not training or self.token_dropout_prob == 0:
            return tokens
        
        batch_size, num_tokens, dim = tokens.shape
        keep_prob = 1.0 - self.token_dropout_prob
        
        # Dropout entire tokens (not individual dims)
        mask = torch.bernoulli(
            torch.full((batch_size, num_tokens, 1), keep_prob, device=tokens.device)
        )
        
        return tokens * mask / keep_prob
    
    def compute_diversity_metrics(self, tokens):
        """
        FIX #9: Monitor diversity for evaluation
        """
        # Pairwise cosine similarity
        normalized = F.normalize(tokens, p=2, dim=-1)
        similarity = torch.bmm(normalized, normalized.transpose(1, 2))
        
        # Remove diagonal
        batch_size, num_tokens, _ = similarity.shape
        mask = ~torch.eye(num_tokens, dtype=torch.bool, device=tokens.device)
        off_diag_sim = similarity[:, mask].view(batch_size, num_tokens, num_tokens - 1)
        
        # Statistics
        mean_sim = off_diag_sim.mean().item()
        max_sim = off_diag_sim.max().item()
        
        # Token std (within each token across batch)
        # ðŸ”¥ FIX: Check batch size to avoid std() warning
        if batch_size > 1:
            token_std = tokens.std(dim=0, unbiased=False).mean().item()
        else:
            token_std = 0.0  # Can't compute std with single sample
        
        return {
            'mean_similarity': mean_sim,
            'max_similarity': max_sim,
            'token_std': token_std,
            'is_collapsed': max_sim > 0.95 or token_std < self.min_std_threshold
        }


# ============================================================================
# FIX #1: BOTTLENECK ENFORCEMENT - Reasoning-only decoder conditioning
# ============================================================================

@dataclass
class FixedVQAOutput:
    """Output with intervention capabilities"""
    answer_logits: torch.Tensor
    reasoning_latents: torch.Tensor
    reasoning_compressed: torch.Tensor  # The actual bottleneck
    answer_loss: Optional[torch.Tensor] = None
    kl_loss: Optional[torch.Tensor] = None
    ortho_loss: Optional[torch.Tensor] = None
    total_loss: Optional[torch.Tensor] = None
    diversity_metrics: Optional[dict] = None
    attention_weights: Optional[torch.Tensor] = None


class FixedLatentReasoningVQA(nn.Module):
    """
    FIXED Latent Reasoning VQA
    
    CRITICAL CHANGES:
    
    1. âœ… BOTTLENECK: Decoder sees ONLY reasoning (not fused_features)
    2. âœ… POSTERIOR COLLAPSE: KL warmup + free bits + stop gradient
    3. âœ… VISION GROUNDING: Vision-first fusion + image dropout
    4. âœ… PROPER SIZE: 4-8 tokens Ã— 256 dim
    5. âœ… DIVERSITY: Orthogonality loss + metrics
    6. âœ… INTERVENTION: Built-in ablation
    7. âœ… CURRICULUM: Stage-based training
    """
    
    def __init__(
        self,
        dinov2_model_name: str = 'facebook/dinov2-base',
        bartpho_model_name: str = 'vinai/bartpho-syllable',
        num_reasoning_tokens: int = 6,  # FIX #4: Small!
        latent_dim: int = 256,  # FIX #4: Compressed!
        num_reasoning_layers: int = 2,
        num_fusion_layers: int = 2,
        num_heads: int = 8,
        dropout: float = 0.1,
        free_bits: float = 0.5,  # FIX #2
        ortho_weight: float = 0.1,  # FIX #5
        image_dropout_prob: float = 0.1,  # FIX #3
        token_dropout_prob: float = 0.3,  # FIX #5
        gradient_checkpointing: bool = True
    ):
        super().__init__()
        
        print("[FIXED MODEL] Initializing with critical fixes...")
        print(f"  âœ… Reasoning bottleneck: {num_reasoning_tokens} tokens Ã— {latent_dim} dim")
        print(f"  âœ… Free bits: {free_bits}")
        print(f"  âœ… Orthogonality: {ortho_weight}")
        print(f"  âœ… Image dropout: {image_dropout_prob}")
        
        # Vision encoder
        self.vision_encoder = AutoModel.from_pretrained(dinov2_model_name)
        vision_hidden_dim = self.vision_encoder.config.hidden_size
        
        # Language model
        bartpho_full = MBartForConditionalGeneration.from_pretrained(bartpho_model_name)
        bartpho_full.config.use_cache = False
        
        self.tokenizer = BartphoTokenizer.from_pretrained(bartpho_model_name)
        bart_hidden_dim = bartpho_full.config.d_model
        
        self.encoder = bartpho_full.model.encoder
        self.decoder = bartpho_full.model.decoder
        self.lm_head = bartpho_full.lm_head
        
        self.config = self.encoder.config
        self.config.decoder_start_token_id = self.tokenizer.bos_token_id
        self.config.pad_token_id = self.tokenizer.pad_token_id
        self.config.eos_token_id = self.tokenizer.eos_token_id
        
        del bartpho_full
        
        # Vision projection
        self.vision_proj = nn.Sequential(
            nn.Linear(vision_hidden_dim, bart_hidden_dim),
            nn.LayerNorm(bart_hidden_dim),
            nn.Dropout(dropout)
        )
        
        # FIX #3: Vision-first fusion
        self.vision_first_fusion = nn.ModuleList([
            VisionFirstFusion(bart_hidden_dim, num_heads, dropout)
            for _ in range(num_fusion_layers)
        ])
        
        # FIX #4: Compressed latent reasoning
        self.latent_reasoning = CompressedLatentReasoning(
            input_dim=bart_hidden_dim,
            num_tokens=num_reasoning_tokens,
            latent_dim=latent_dim,
            num_heads=num_heads,
            num_layers=num_reasoning_layers,
            dropout=dropout,
            free_bits=free_bits
        )
        
        # FIX #5: Diversity regularizer
        self.diversity_regularizer = DiversityRegularizer(
            ortho_weight=ortho_weight,
            token_dropout_prob=token_dropout_prob
        )
        
        # Config
        self.image_dropout_prob = image_dropout_prob
        self.num_reasoning_tokens = num_reasoning_tokens
        self.latent_dim = latent_dim
        
        # Gradient checkpointing
        # NOTE: Disabled decoder checkpointing to avoid "decoder_input_ids and decoder_inputs_embeds" conflict
        if gradient_checkpointing:
            self.vision_encoder.gradient_checkpointing_enable()
            self.encoder.gradient_checkpointing_enable()
            # self.decoder.gradient_checkpointing_enable()  # Disabled - causes conflict
        
        print("[FIXED MODEL] âœ“ Initialization complete")
    
    def freeze_pretrained(self, unfreeze_encoder_layers: int = 3, unfreeze_decoder: bool = True):
        """Freeze pretrained components"""
        # Freeze vision
        for param in self.vision_encoder.parameters():
            param.requires_grad = False
        
        # Freeze encoder except last N
        for param in self.encoder.parameters():
            param.requires_grad = False
        
        total_layers = len(self.encoder.layers)
        for i, layer in enumerate(self.encoder.layers):
            if i >= total_layers - unfreeze_encoder_layers:
                for param in layer.parameters():
                    param.requires_grad = True
        
        # FIX #8: Freeze decoder only in Stage 1 (curriculum)
        # For Stage 2-3, decoder should be trainable!
        if unfreeze_decoder:
            # Unfreeze last 2 layers of decoder for fine-tuning
            total_decoder_layers = len(self.decoder.layers)
            for param in self.decoder.parameters():
                param.requires_grad = False
            
            for i, layer in enumerate(self.decoder.layers):
                if i >= total_decoder_layers - 2:  # Last 2 layers
                    for param in layer.parameters():
                        param.requires_grad = True
        else:
            # Completely freeze decoder (Stage 1 only)
            for param in self.decoder.parameters():
                param.requires_grad = False
        
        # Trainable: fusion + reasoning + lm_head + decoder (if unfrozen)
        trainable = (
            sum(p.numel() for p in self.vision_proj.parameters()) +
            sum(p.numel() for p in self.vision_first_fusion.parameters()) +
            sum(p.numel() for p in self.latent_reasoning.parameters()) +
            sum(p.numel() for p in self.lm_head.parameters()) +
            sum(p.numel() for p in self.encoder.parameters() if p.requires_grad) +
            sum(p.numel() for p in self.decoder.parameters() if p.requires_grad)
        )
        
        print(f"[FIXED MODEL] Trainable params: {trainable/1e6:.1f}M")
        print(f"[FIXED MODEL] Decoder trainable: {unfreeze_decoder}")
    
    def forward(
        self,
        pixel_values: torch.Tensor,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        labels: Optional[torch.Tensor] = None,
        deterministic_reasoning: bool = False,
        # FIX #6: Intervention controls
        ablate_reasoning: bool = False,
        noise_reasoning: Optional[float] = None,
        # FIX #2: Training curriculum
        stop_gradient_to_latent: bool = False,
        # FIX #8: KL warmup
        kl_weight: float = 1.0,
        # PROPOSAL: Stochastic sampling for teacher distillation
        temperature: float = 1.0  # Temperature for stochastic reasoning sampling
    ):
        """
        Forward pass with interventions and stochastic sampling
        
        Args:
            ablate_reasoning: Zero out reasoning (test if model depends on it)
            noise_reasoning: Add noise to test robustness
            stop_gradient_to_latent: Prevent decoder from influencing latent
            kl_weight: Curriculum for KL (warmup from 0 â†’ 1)
            temperature: Temperature for sampling reasoning (>1 = more random, <1 = more deterministic)
        """
        # 1. Encode vision
        visual_outputs = self.vision_encoder(pixel_values, return_dict=True)
        visual_features = self.vision_proj(visual_outputs.last_hidden_state)
        
        # 2. Encode text
        text_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
        text_features = text_outputs.last_hidden_state
        
        # 3. FIX #3: Vision-first fusion with image dropout
        fused = text_features
        attention_maps = []
        for fusion_layer in self.vision_first_fusion:
            fused, attn = fusion_layer(fused, visual_features, self.image_dropout_prob)
            attention_maps.append(attn)
        
        # 4. FIX #4 & #2: Extract compressed reasoning with free bits + PROPOSAL temperature
        reasoning_latents, kl_loss, compressed_z, mu, logvar = self.latent_reasoning(
            fused, attention_mask,
            deterministic=deterministic_reasoning,
            stop_gradient=stop_gradient_to_latent,
            temperature=temperature  # PROPOSAL: Stochastic sampling
        )
        
        # 5. FIX #5: Apply diversity regularization
        reasoning_latents = self.diversity_regularizer.apply_token_dropout(
            reasoning_latents, training=self.training
        )
        
        ortho_loss = self.diversity_regularizer.compute_orthogonality_loss(reasoning_latents)
        
        # 6. FIX #6: Interventions
        if ablate_reasoning:
            reasoning_latents = torch.zeros_like(reasoning_latents)
        
        if noise_reasoning is not None:
            reasoning_latents = reasoning_latents + torch.randn_like(reasoning_latents) * noise_reasoning
        
        # 7. FIX #1: CRITICAL - Decoder sees ONLY reasoning (bottleneck!)
        encoder_hidden_states = reasoning_latents  # NOT concat with fused!
        
        # 8. Decode
        # NOTE: Skip decoder if no labels (will use generate_from_reasoning() instead)
        if labels is not None:
            decoder_input_ids = shift_tokens_right(
                labels, self.config.pad_token_id, self.config.decoder_start_token_id
            )
            
            decoder_outputs = self.decoder(
                input_ids=decoder_input_ids,
                attention_mask=(labels != self.config.pad_token_id),
                encoder_hidden_states=encoder_hidden_states,
                return_dict=True,
                use_cache=False
            )
            
            logits = self.lm_head(decoder_outputs.last_hidden_state)
        else:
            # No labels = no decoder forward (use generate_from_reasoning() instead)
            logits = None
        
        # 9. Losses
        answer_loss = None
        total_loss = None
        
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            answer_loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))
            
            # FIX #8: Curriculum - gradually increase KL weight
            total_loss = (
                answer_loss +
                kl_weight * 0.01 * kl_loss +  # Warmup KL
                ortho_loss * self.diversity_regularizer.ortho_weight
            )
        
        # FIX #9: Diversity metrics for monitoring
        diversity_metrics = None
        if not self.training:
            diversity_metrics = self.diversity_regularizer.compute_diversity_metrics(reasoning_latents)
        
        return FixedVQAOutput(
            answer_logits=logits,
            reasoning_latents=reasoning_latents,
            reasoning_compressed=compressed_z,
            answer_loss=answer_loss,
            kl_loss=kl_loss,
            ortho_loss=ortho_loss,
            total_loss=total_loss,
            diversity_metrics=diversity_metrics,
            attention_weights=attention_maps[-1] if attention_maps else None
        )
    
    @torch.no_grad()
    def generate(
        self, 
        pixel_values, 
        input_ids, 
        attention_mask, 
        max_length=32, 
        num_beams=4,
        # FIX #6: Intervention during generation
        ablate_reasoning=False,
        noise_reasoning=None
    ):
        """Generate with intervention support"""
        # Encode
        visual_outputs = self.vision_encoder(pixel_values, return_dict=True)
        visual_features = self.vision_proj(visual_outputs.last_hidden_state)
        
        text_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
        text_features = text_outputs.last_hidden_state
        
        # Fuse
        fused = text_features
        for fusion_layer in self.vision_first_fusion:
            fused, _ = fusion_layer(fused, visual_features, image_dropout_prob=0.0)
        
        # Reasoning (deterministic)
        reasoning_latents, _, _, _, _ = self.latent_reasoning(
            fused, attention_mask, deterministic=True, stop_gradient=False
        )
        
        # Intervention
        if ablate_reasoning:
            reasoning_latents = torch.zeros_like(reasoning_latents)
        
        if noise_reasoning is not None:
            reasoning_latents = reasoning_latents + torch.randn_like(reasoning_latents) * noise_reasoning
        
        # FIX #1: Only reasoning to decoder
        encoder_hidden_states = reasoning_latents
        
        # Generate
        batch_size = pixel_values.size(0)
        decoder_input_ids = torch.full(
            (batch_size, 1), self.tokenizer.bos_token_id,
            dtype=torch.long, device=pixel_values.device
        )
        
        generated_ids = self.decoder.generate(
            input_ids=decoder_input_ids,
            encoder_hidden_states=encoder_hidden_states,
            max_length=max_length,
            num_beams=num_beams,
            pad_token_id=self.config.pad_token_id,
            eos_token_id=self.config.eos_token_id,
            bos_token_id=self.tokenizer.bos_token_id,
            use_cache=True
        )
        
        answers = [
            self.tokenizer.decode(ids, skip_special_tokens=True).strip()
            for ids in generated_ids
        ]
        
        return answers
    
    def generate_from_reasoning(
        self,
        reasoning_latents: torch.Tensor,
        max_length: int = 10,  # VQA answers are short (1-3 words typically)
        num_beams: int = 1  # Greedy by default for training speed
    ):
        """
        Generate answers from pre-computed reasoning latents.
        Used during training to avoid re-encoding.
        
        Args:
            reasoning_latents: Pre-computed reasoning representations [batch, num_tokens, dim]
            max_length: Maximum generation length (default 10 for short VQA answers)
            num_beams: Number of beams (1 = greedy, >1 = beam search)
        
        Returns:
            List of decoded answer strings
        """
        batch_size = reasoning_latents.size(0)
        device = reasoning_latents.device
        
        # Start with BOS token
        generated_ids = torch.full(
            (batch_size, 1), 
            self.tokenizer.bos_token_id,
            dtype=torch.long, 
            device=device
        )
        
        # ðŸ”¥ FIX: Manual autoregressive decoding (MBartDecoder doesn't have .generate())
        for _ in range(max_length - 1):
            # Get decoder outputs
            decoder_outputs = self.decoder(
                input_ids=generated_ids,
                encoder_hidden_states=reasoning_latents,
                use_cache=False,
                return_dict=True
            )
            
            # Get logits for next token
            hidden_states = decoder_outputs.last_hidden_state
            logits = self.lm_head(hidden_states[:, -1:, :])  # [batch, 1, vocab_size]
            
            # Sample next token (greedy decoding)
            next_token = logits.argmax(dim=-1)  # [batch, 1]
            
            # Append to sequence
            generated_ids = torch.cat([generated_ids, next_token], dim=1)
            
            # Stop if all sequences hit EOS
            if (next_token == self.config.eos_token_id).all():
                break
        
        # Decode to strings
        answers = [
            self.tokenizer.decode(ids, skip_special_tokens=True).strip()
            for ids in generated_ids
        ]
        
        return answers


# ============================================================================
# TEACHER EVALUATOR (MISSING! - CRITICAL FOR PROPOSAL!)
# ============================================================================

class TeacherEvaluator:
    """
    Teacher model for online distillation (PROPOSAL Section 6 & 7)
    
    Provides answer quality scores to guide reasoning module
    Supports:
    - Rule-based (fast baseline)
    - VLM-based (Qwen2.5-VL-7B-Instruct for semantic understanding)
    """
    
    def __init__(
        self, 
        teacher_type: str = 'rule_based', 
        device: str = 'cuda',
        tokenizer = None
    ):
        self.teacher_type = teacher_type
        self.device = device
        self.tokenizer = tokenizer
        
        print(f"[Teacher] Initializing {teacher_type} evaluator...")
        
        if teacher_type == 'vlm':
            try:
                from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
                
                print("[Teacher] Loading Qwen2.5-VL-7B-Instruct...")
                # Load on GPU with optimization for speed
                self.vlm_model = Qwen2VLForConditionalGeneration.from_pretrained(
                    "Qwen/Qwen2-VL-7B-Instruct",
                    torch_dtype=torch.float16,
                    device_map="auto",  # Auto GPU allocation
                    low_cpu_mem_usage=True
                )
                self.vlm_model.eval()  # Inference mode
                
                self.vlm_processor = AutoProcessor.from_pretrained(
                    "Qwen/Qwen2-VL-7B-Instruct"
                )
                
                print("[Teacher] âœ… Qwen2.5-VL loaded on GPU (faster inference)")
                
            except Exception as e:
                print(f"[Teacher] âŒ Failed to load VLM: {e}")
                print("[Teacher] Falling back to rule-based")
                self.teacher_type = 'rule_based'
        else:
            print("[Teacher] âœ… Rule-based evaluator ready")
    
    @torch.no_grad()
    def evaluate_answers(
        self,
        predictions: list,
        ground_truths: list,
        images: Optional[torch.Tensor] = None,
        questions: Optional[list] = None
    ) -> torch.Tensor:
        """
        Evaluate answer quality (PROPOSAL: teacher judges answer plausibility)
        
        Args:
            predictions: Model predictions (strings)
            ground_truths: Ground truth answers (strings)
            images: Optional image tensors for VLM [B, C, H, W]
            questions: Optional question texts for VLM
        
        Returns:
            scores: Quality scores [0, 1] for each prediction
        """
        if self.teacher_type == 'vlm' and hasattr(self, 'vlm_model'):
            return self._evaluate_with_vlm(predictions, ground_truths, images, questions)
        else:
            return self._evaluate_rule_based(predictions, ground_truths)
    
    def _evaluate_rule_based(
        self, 
        predictions: list, 
        ground_truths: list
    ) -> torch.Tensor:
        """Rule-based evaluation (fast, simple)"""
        scores = []
        
        for pred, gt in zip(predictions, ground_truths):
            pred_norm = pred.lower().strip()
            gt_norm = gt.lower().strip()
            
            # Exact match
            if pred_norm == gt_norm:
                score = 1.0
            # Partial match
            elif gt_norm in pred_norm or pred_norm in gt_norm:
                score = 0.7
            # Semantic similarity
            elif self._semantic_similarity(pred_norm, gt_norm) > 0.5:
                score = 0.5
            # Wrong
            else:
                score = 0.0
            
            scores.append(score)
        
        return torch.tensor(scores, dtype=torch.float32, device=self.device)
    
    def _evaluate_with_vlm(
        self,
        predictions: list,
        ground_truths: list,
        images: Optional[torch.Tensor] = None,
        questions: Optional[list] = None
    ) -> torch.Tensor:
        """
        VLM-based evaluation (Qwen2.5-VL)
        
        Uses vision-language model to score answer quality
        More semantic understanding than rule-based
        """
        scores = []
        
        for i, (pred, gt) in enumerate(zip(predictions, ground_truths)):
            # Construct prompt for VLM
            if images is not None and questions is not None:
                # Full multimodal evaluation
                prompt = self._construct_vlm_prompt(
                    question=questions[i] if i < len(questions) else "",
                    prediction=pred,
                    ground_truth=gt
                )
                
                # Prepare inputs
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "image": images[i] if i < len(images) else None
                            },
                            {
                                "type": "text",
                                "text": prompt
                            }
                        ]
                    }
                ]
                
                # Process
                text = self.vlm_processor.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
                
                image_inputs = None
                if images is not None and i < len(images):
                    # Convert tensor to PIL
                    import torchvision.transforms as T
                    to_pil = T.ToPILImage()
                    pil_image = to_pil(images[i].cpu())
                    image_inputs = [pil_image]
                
                inputs = self.vlm_processor(
                    text=[text],
                    images=image_inputs,
                    return_tensors="pt"
                ).to(self.vlm_model.device)
                
                # Generate score
                outputs = self.vlm_model.generate(
                    **inputs,
                    max_new_tokens=10,
                    temperature=0.1  # Low temp for consistent scoring
                )
                
                # Decode and parse score
                response = self.vlm_processor.batch_decode(
                    outputs, skip_special_tokens=True
                )[0]
                
                # Extract score from response
                score = self._parse_vlm_score(response)
                
            else:
                # Text-only evaluation (fallback)
                score = self._evaluate_text_only_vlm(pred, gt)
            
            scores.append(score)
        
        return torch.tensor(scores, dtype=torch.float32, device=self.device)
    
    def _construct_vlm_prompt(
        self, 
        question: str, 
        prediction: str, 
        ground_truth: str
    ) -> str:
        """Construct prompt for VLM evaluation"""
        prompt = f"""You are evaluating the quality of a Vietnamese VQA model's answer.

Question: {question}
Student Answer: {prediction}
Ground Truth: {ground_truth}

Rate the student answer quality from 0 to 100:
- 100: Perfect match (semantically identical to ground truth)
- 70-90: Correct but different wording
- 40-70: Partially correct
- 0-40: Incorrect

Respond with ONLY a number (0-100).
Score:"""
        return prompt
    
    def _parse_vlm_score(self, response: str) -> float:
        """Parse VLM response to extract score"""
        import re
        
        # Extract number from response
        numbers = re.findall(r'\d+', response)
        
        if numbers:
            score = int(numbers[0])
            # Normalize to [0, 1]
            return min(max(score / 100.0, 0.0), 1.0)
        else:
            # Fallback: parse text
            response_lower = response.lower()
            if any(word in response_lower for word in ['perfect', 'correct', 'excellent']):
                return 0.9
            elif any(word in response_lower for word in ['good', 'mostly']):
                return 0.7
            elif any(word in response_lower for word in ['partial', 'somewhat']):
                return 0.5
            else:
                return 0.2
    
    def _evaluate_text_only_vlm(self, pred: str, gt: str) -> float:
        """Text-only VLM evaluation (without image)"""
        prompt = f"""Rate answer quality (0-100):
Ground truth: {gt}
Prediction: {pred}

Score (number only):"""
        
        inputs = self.vlm_processor(
            text=[prompt],
            return_tensors="pt"
        ).to(self.vlm_model.device)
        
        outputs = self.vlm_model.generate(**inputs, max_new_tokens=5)
        response = self.vlm_processor.batch_decode(outputs, skip_special_tokens=True)[0]
        
        return self._parse_vlm_score(response)
    
    def _semantic_similarity(self, s1: str, s2: str) -> float:
        """Simple word overlap similarity (for rule-based)"""
        words1 = set(s1.split())
        words2 = set(s2.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0


# ============================================================================
# FIX #8: TRAINING CURRICULUM
# ============================================================================

class TrainingCurriculum:
    """
    FIX #8: Simplified training dynamics
    
    Stage 1: Answer-only (no reasoning)
    Stage 2: Warmup reasoning (KL warmup, no teacher)
    Stage 3: Full (with teacher)
    """
    def __init__(self, total_steps_per_stage: int = 1000, max_kl_weight: float = 15.0):
        """
        Args:
            total_steps_per_stage: Total steps for ENTIRE STAGE 2 (not per epoch!)
                                  Should be: batches_per_epoch * num_stage2_epochs
            max_kl_weight: Maximum KL weight (default 15.0)
                          Note: Loss uses `kl_weight * 0.01 * kl_loss`
                          So effective weight = 15.0 * 0.01 = 0.15
                          Target KL contribution: ~0.05-0.1
        """
        self.total_steps = total_steps_per_stage
        self.current_step = 0
        self.max_kl_weight = max_kl_weight
    
    def get_kl_weight(self, stage: int):
        """
        FIX #2: KL warmup to prevent collapse
        
        Stage 1: KL = 0 (no reasoning)
        Stage 2: KL = 0 â†’ max_kl_weight (gradual warmup)
        Stage 3: KL = max_kl_weight (full)
        """
        if stage == 1:
            return 0.0
        elif stage == 2:
            # Linear warmup to max_kl_weight
            progress = min(self.current_step / self.total_steps, 1.0)
            return progress * self.max_kl_weight
        else:  # stage 3
            return self.max_kl_weight
    
    def get_stop_gradient(self, stage: int):
        """
        FIX #2: Stop gradient in early stages
        """
        return stage == 1  # Stop gradient in baseline stage
    
    def step(self):
        self.current_step += 1


if __name__ == '__main__':
    print("="*80)
    print("FIXED LATENT REASONING VQA - ALL CRITICAL ISSUES ADDRESSED")
    print("="*80)
    print("\nFIXES APPLIED:")
    print("  1. âœ… Bottleneck: Decoder sees ONLY reasoning")
    print("  2. âœ… Posterior collapse: Free bits + KL warmup + stop gradient")
    print("  3. âœ… Vision grounding: Vision-first + image dropout")
    print("  4. âœ… Latent size: 4-8 tokens Ã— 256 dim")
    print("  5. âœ… Diversity: Orthogonality + metrics")
    print("  6. âœ… Intervention: Built-in ablation")
    print("  7. âœ… Dataset: Filter hard examples (in training script)")
    print("  8. âœ… Curriculum: Stage-based training")
    print("  9. âœ… Metrics: Intervention tests")
    print("="*80)
    
    # Test model
    model = FixedLatentReasoningVQA(
        num_reasoning_tokens=6,
        latent_dim=256,
        free_bits=0.5,
        ortho_weight=0.1,
        image_dropout_prob=0.1
    )
    
    print(f"\nTotal params: {sum(p.numel() for p in model.parameters())/1e6:.1f}M")
    print("Model ready for training with ALL fixes applied! ðŸŽ‰")
