# L·ªÜNH CH·∫†Y DETERMINISTIC VQA (No Latent) - VERSION 2.0

## üéâ **NEW IN VERSION 2.0**

- ‚úÖ **FIXED beam search** (was broken, now uses HuggingFace's proper implementation)
- ‚úÖ **LR scheduler** (ReduceLROnPlateau ho·∫∑c Cosine)
- ‚úÖ **Early stopping** (t·ª± ƒë·ªông d·ª´ng khi overfitting)
- ‚úÖ **F1 score** (metrics t·ªët h∆°n exact match)
- ‚úÖ **Label smoothing** (prevent overfitting)
- ‚úÖ **Dataset analysis** (detect imbalance)

---

## üöÄ BASIC COMMANDS

### 1. Ch·∫°y training M·∫∂C ƒê·ªäNH (with improvements!)
```bash
python train_no_latent.py --early_stopping --scheduler plateau
```

### 2. Ch·∫°y training FULL FEATURES
```bash
python train_no_latent.py \
  --batch_size 12 \
  --epochs 30 \
  --lr 5e-5 \
  --scheduler plateau \
  --early_stopping \
  --analyze_dataset \
  --sample_every 3
```

### 3. Ch·∫°y training n·ªÅn (background)
```bash
nohup python train_no_latent.py > logs/train.log 2>&1 &
```

### 4. Ch·∫°y training v·ªõi nhi·ªÅu options
```bash
python train_no_latent.py \
  --data_dir ./data \
  --batch_size 12 \
  --epochs 30 \
  --lr 5e-5 \
  --weight_decay 0.01 \
  --num_fusion_layers 2 \
  --output_dir ./checkpoints_no_latent \
  --sample_every 3
```

---

## üìã T·∫§T C·∫¢ ARGUMENTS

### **Data Arguments**
```bash
--data_dir PATH           # Th∆∞ m·ª•c data (default: ./data)
--batch_size INT          # Batch size (default: 12)
--num_workers INT         # S·ªë workers cho dataloader (default: 4)
```

### **Model Arguments**
```bash
--dinov2_model NAME       # DINOv2 model (default: facebook/dinov2-base)
--bartpho_model NAME      # BARTpho model (default: vinai/bartpho-syllable)
--num_fusion_layers INT   # S·ªë Flamingo layers (default: 2)
--num_heads INT           # S·ªë attention heads (default: 8)
--dropout FLOAT           # Dropout rate (default: 0.1)
```

### **Training Arguments**
```bash
--epochs INT              # S·ªë epochs (default: 30)
--lr FLOAT                # Learning rate (default: 5e-5)
--weight_decay FLOAT      # Weight decay (default: 0.01)
--max_norm FLOAT          # Gradient clipping (default: 1.0)
--no_amp                  # T·∫Øt mixed precision
```

### **üî• NEW: LR Scheduler & Early Stopping**
```bash
--scheduler {none,plateau,cosine}  # LR scheduler (default: plateau)
--scheduler_patience INT           # Patience cho plateau (default: 3)
--scheduler_factor FLOAT           # Factor cho plateau (default: 0.5)
--early_stopping                   # Enable early stopping
--early_stopping_patience INT      # Early stop patience (default: 5)
```

### **Freezing Arguments**
```bash
--unfreeze_encoder_layers INT  # S·ªë text encoder layers unfreeze (default: 3)
--freeze_decoder              # Freeze decoder (default: unfrozen)
```

### **Checkpoint Arguments**
```bash
--output_dir PATH         # Th∆∞ m·ª•c l∆∞u checkpoints (default: ./checkpoints_no_latent)
--resume PATH             # Resume t·ª´ checkpoint
--save_every INT          # L∆∞u checkpoint m·ªói N epochs (default: 1)
--sample_every INT        # Sample predictions m·ªói N epochs (default: 3)
```

### **Misc Arguments**
```bash
--seed INT                # Random seed (default: 42)
--no_gradient_checkpointing  # T·∫Øt gradient checkpointing
--analyze_dataset         # üî• NEW: Analyze dataset tr∆∞·ªõc training (detect imbalance!)
```

---

## üéØ C√ÅC L·ªÜNH PH·ªî BI·∫æN (V2.0)

### üî• RECOMMENDED: Training v·ªõi t·∫•t c·∫£ improvements
```bash
python train_no_latent.py \
  --batch_size 12 \
  --epochs 30 \
  --lr 5e-5 \
  --scheduler plateau \
  --scheduler_patience 3 \
  --early_stopping \
  --early_stopping_patience 5 \
  --analyze_dataset \
  --sample_every 3
```

### Training nhanh (test run)
```bash
python train_no_latent.py \
  --epochs 10 \
  --batch_size 8 \
  --scheduler plateau \
  --early_stopping \
  --sample_every 2
```

### Training v·ªõi learning rate cao h∆°n
```bash
python train_no_latent.py --lr 1e-4
```

### Training v·ªõi nhi·ªÅu fusion layers
```bash
python train_no_latent.py --num_fusion_layers 4
```

### Resume training t·ª´ checkpoint
```bash
python train_no_latent.py --resume checkpoints_no_latent/checkpoint_stage3_epoch10.pt
```

### Training ƒë·ªÉ debug (save √≠t h∆°n)
```bash
python train_no_latent.py \
  --epochs 5 \
  --batch_size 8 \
  --save_every 2 \
  --sample_every 1
```

### Training full power (max settings)
```bash
python train_no_latent.py \
  --batch_size 20 \
  --epochs 50 \
  --lr 1e-4 \
  --num_fusion_layers 4 \
  --unfreeze_encoder_layers 6
```

### Training cho low memory (16GB GPU)
```bash
python train_no_latent.py \
  --batch_size 8 \
  --num_workers 2
```

---

## üìä EVALUATION COMMANDS

### Eval validation set
```bash
python eval_no_latent.py \
  --checkpoint checkpoints_no_latent/best_model.pt \
  --split val
```

### Eval test set
```bash
python eval_no_latent.py \
  --checkpoint checkpoints_no_latent/best_model.pt \
  --split test
```

### Eval v·ªõi nhi·ªÅu samples
```bash
python eval_no_latent.py \
  --checkpoint checkpoints_no_latent/best_model.pt \
  --split val \
  --num_samples 50 \
  --batch_size 16
```

---

## üîç MONITORING COMMANDS

### Xem help (t·∫•t c·∫£ arguments)
```bash
python train_no_latent.py --help
```

### Ki·ªÉm tra GPU
```bash
nvidia-smi
watch -n 1 nvidia-smi  # Auto refresh m·ªói 1s
```

### Xem log realtime
```bash
tail -f logs/train.log
```

### Ki·ªÉm tra training ƒëang ch·∫°y
```bash
ps aux | grep train_no_latent
```

### Kill training
```bash
pkill -f train_no_latent.py
```

---

## üÜö SO S√ÅNH K·∫æT QU·∫¢

### Compare models
```bash
python compare_models.py
```

---

## üí° EXAMPLES TH·ª∞C T·∫æ

### Example 1: Quick test (5 epochs)
```bash
python train_no_latent.py \
  --epochs 5 \
  --batch_size 8 \
  --sample_every 1 \
  --output_dir ./test_checkpoints
```

### Example 2: Full training
```bash
mkdir -p logs checkpoints_no_latent

nohup python train_no_latent.py \
  --epochs 30 \
  --batch_size 12 \
  --lr 5e-5 \
  --num_fusion_layers 2 \
  --output_dir ./checkpoints_no_latent \
  > logs/train_no_latent.log 2>&1 &

# Xem log
tail -f logs/train_no_latent.log
```

### Example 3: Resume sau khi b·ªã crash
```bash
python train_no_latent.py \
  --resume checkpoints_no_latent/checkpoint_stage3_epoch15.pt \
  --epochs 30
```

### Example 4: Experiment v·ªõi config m·ªõi
```bash
python train_no_latent.py \
  --epochs 25 \
  --batch_size 16 \
  --lr 8e-5 \
  --num_fusion_layers 3 \
  --dropout 0.15 \
  --output_dir ./checkpoints_experiment1
```

---

## üéì TIPS

1. **Ch·∫°y song song nhi·ªÅu experiments:**
   ```bash
   # Experiment 1
   python train_no_latent.py --lr 5e-5 --output_dir ./exp1 &
   
   # Experiment 2
   python train_no_latent.py --lr 1e-4 --output_dir ./exp2 &
   ```

2. **Monitor GPU usage:**
   ```bash
   watch -n 1 'nvidia-smi && echo && ps aux | grep train'
   ```

3. **Auto restart n·∫øu crash:**
   ```bash
   while true; do
       python train_no_latent.py || sleep 10
   done
   ```

---

## ‚ùì HELP

ƒê·ªÉ xem t·∫•t c·∫£ options:
```bash
python train_no_latent.py -h
```

Output:
```
usage: train_no_latent.py [-h] [--data_dir DATA_DIR] [--batch_size BATCH_SIZE]
                          [--num_workers NUM_WORKERS] [--dinov2_model DINOV2_MODEL]
                          [--bartpho_model BARTPHO_MODEL]
                          [--num_fusion_layers NUM_FUSION_LAYERS] [--num_heads NUM_HEADS]
                          [--dropout DROPOUT] [--epochs EPOCHS] [--lr LR]
                          [--weight_decay WEIGHT_DECAY] [--max_norm MAX_NORM] [--no_amp]
                          [--unfreeze_encoder_layers UNFREEZE_ENCODER_LAYERS]
                          [--freeze_decoder] [--output_dir OUTPUT_DIR] [--resume RESUME]
                          [--save_every SAVE_EVERY] [--sample_every SAMPLE_EVERY]
                          [--seed SEED] [--no_gradient_checkpointing]

Train Deterministic VQA (No Latent)
...
```
