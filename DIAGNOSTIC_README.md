# ğŸ”¬ VQA Model Diagnostic Tools

## Triáº¿t lÃ½: Evidence-Based Debugging

**KHÃ”NG BAO GIá»œ** thay Ä‘á»•i model dá»±a trÃªn triá»‡u chá»©ng (symptoms)!

**LUÃ”N LUÃ”N** cháº¡y diagnostics Ä‘á»ƒ tÃ¬m root cause trÆ°á»›c khi Ä‘á»•i architecture.

---

## ğŸ“‹ Quick Start

```bash
# Cháº¡y vá»›i validation set
python diagnostic_tools.py \
    --checkpoint checkpoints/best_model.pt \
    --csv data/val.csv \
    --image_dir data/images \
    --batch_size 16

# Hoáº·c cháº¡y vá»›i test set (náº¿u khÃ´ng cÃ³ val)
python diagnostic_tools.py \
    --checkpoint checkpoints/best_model.pt \
    --csv data/test.csv \
    --image_dir data/images \
    --batch_size 16
```

**Thá»i gian cháº¡y:** ~5-10 phÃºt (tÃ¹y dataset size)

**Output:** 
- âœ… Clear recommendation (FIX GATE / CHANGE ENCODER / FIX DATA)
- ğŸ“Š Detailed statistics for each test
- ğŸ¯ Actionable next steps

---

## ğŸ§ª Test Suite Overview

### Test A: Gate Behavior Analysis
**Má»¥c Ä‘Ã­ch:** Kiá»ƒm tra xem gating cÃ³ Ä‘ang suppress vision khÃ´ng

**Metric:** Mean gate value across all patches

**Thresholds:**
- `< 0.3` â†’ âŒ Vision suppressed (FIX GATE!)
- `0.3-0.5` â†’ âš ï¸  Low vision usage
- `0.5-0.7` â†’ âœ… Balanced fusion
- `> 0.7` â†’ âœ… Strong vision reliance

**Recommendation if fail:**
1. Increase `vision_gate_init` (3.0 thay vÃ¬ 1.5)
2. Add gate regularization loss
3. Remove gating (`use_vision_gate=False`)

---

### Test B: Vision Dependency (Ablation)
**Má»¥c Ä‘Ã­ch:** Äo lÆ°á»ng model phá»¥ thuá»™c vision bao nhiÃªu

**Method:** Compare accuracy:
- Real images vs Blank images
- Real images vs Noise images

**Metric:** Accuracy drop khi remove vision

**Thresholds:**
- `< 10%` â†’ âŒ Model khÃ´ng cáº§n vision (BAD!)
- `10-25%` â†’ âš ï¸  Moderate vision usage
- `> 25%` â†’ âœ… Strong vision dependency

**Diagnosis tree:**
```
Drop < 10%
  â”œâ”€ IF feature quality GOOD (Test C pass)
  â”‚  â””â”€> PROBLEM: Dataset bias
  â”‚     ACTION: Fix data, khÃ´ng Ä‘á»•i encoder
  â”‚
  â””â”€ IF feature quality BAD (Test C fail)
     â””â”€> PROBLEM: DINOv2 khÃ´ng extract tá»‘t
        ACTION: Äá»•i sang CLIP/SigLIP
```

---

### Test C: Vision Feature Quality
**Má»¥c Ä‘Ã­ch:** Kiá»ƒm tra DINOv2 cÃ³ extract features tá»‘t khÃ´ng

**Metrics:**
- Feature std (variance across features)
- Feature diversity (pairwise distance across samples)
- NaN/Inf detection

**Thresholds:**
- Std `< 0.1` â†’ âŒ Feature collapse
- Diversity `< 1.0` â†’ âš ï¸  Low diversity
- Has NaN/Inf â†’ âŒ CRITICAL BUG

**Recommendation if fail:**
1. Fine-tune vision encoder (LoRA)
2. Check preprocessing (normalization)
3. Consider different encoder (CLIP, SigLIP)

---

### Test D: Per-Type Breakdown
**Má»¥c Ä‘Ã­ch:** Identify which question types fail

**Output:** Accuracy for:
- OBJECT (ÄÃ¢y lÃ  gÃ¬?)
- COUNT (CÃ³ bao nhiÃªu?)
- COLOR (MÃ u gÃ¬?)
- LOCATION (á» Ä‘Ã¢u?)

**Use case:** 
- If COUNT fails â†’ Need better global features
- If COLOR fails â†’ Need semantic understanding
- If LOCATION fails â†’ Need spatial reasoning

---

## ğŸ¯ Decision Tree (TÃ³m táº¯t)

```
1. Run diagnostic_tools.py

2. Check output recommendation:

   â”Œâ”€ "FIX GATING" 
   â”‚  â””â”€> Increase vision_gate_init
   â”‚     Hoáº·c remove gating
   â”‚     âŒ KHÃ”NG Ä‘á»•i encoder
   â”‚
   â”œâ”€ "CHANGE ENCODER"
   â”‚  â””â”€> DINOv2 features xáº¥u
   â”‚     â†’ Äá»•i sang CLIP/SigLIP
   â”‚     âœ… CÃ³ evidence rÃµ rÃ ng
   â”‚
   â”œâ”€ "FIX DATA"
   â”‚  â””â”€> Dataset bias (text-only Ä‘á»§ máº¡nh)
   â”‚     â†’ Harder questions
   â”‚     â†’ Data augmentation
   â”‚     âŒ KHÃ”NG Ä‘á»•i encoder
   â”‚
   â””â”€ "DINOv2 WORKING WELL"
      â””â”€> Encoder tá»‘t rá»“i
          â†’ Fix fusion/decoder
          âŒ KHÃ”NG Ä‘á»•i encoder
```

---

## ğŸ“Š Example Output

```
================================================================================
ğŸ”¬ VQA MODEL DIAGNOSTIC SUITE
================================================================================
Checkpoint: checkpoints/best_model.pt
Eval CSV: data/test.csv
Image Dir: data/images
================================================================================

ğŸ“¥ Loading model...
ğŸ“¥ Loading evaluation dataset...
âœ… Loaded 1000 evaluation samples

================================================================================
TEST A: VISION GATE BEHAVIOR ANALYSIS
================================================================================
ğŸ“Š Gate Statistics (across 256000 patch-level gates):
   Mean:   0.234
   Median: 0.189
   Std:    0.156
   Min:    0.012
   Max:    0.892
   P25:    0.134
   P75:    0.298
   P90:    0.445

ğŸ” Diagnosis:
   âŒ PROBLEM: Vision is heavily suppressed (mean < 0.3)
   â†’ Model is NOT using vision features effectively
   â†’ ACTION: Fix gating mechanism or remove gate

================================================================================
TEST B: VISION DEPENDENCY (ABLATION TEST)
================================================================================
ğŸ“Š Vision Dependency Results:
   Accuracy with REAL images:  45.2%
   Accuracy with BLANK images: 42.1%
   Accuracy with NOISE images: 41.8%
   
   Drop when removing vision (blank): 3.1%
   Drop when adding noise:            3.4%

ğŸ” Diagnosis:
   âŒ PROBLEM: Model doesn't rely on vision (drop < 10%)
   â†’ Text-only is sufficient for high accuracy
   â†’ Possible causes:
      1. Dataset bias (questions answerable from text)
      2. Vision features not informative
      3. Gating suppressing vision
   â†’ ACTION: Run Test C to check feature quality

================================================================================
ğŸ¯ FINAL RECOMMENDATION
================================================================================
âŒ DON'T change vision encoder yet!
âœ… ACTION: Fix gating mechanism
   Reason: Gating is suppressing vision features
   Try:
      1. Increase vision_gate_init (e.g., 3.0 instead of 1.5)
      2. Add gate regularization loss
      3. Remove gating entirely (use_vision_gate=False)
================================================================================
```

---

## ğŸ› ï¸ Advanced Usage

### Test individual components

```python
from diagnostic_tools import (
    analyze_gate_statistics_v2,
    test_vision_dependency,
    check_vision_feature_quality,
    analyze_per_type_performance
)

# Load your model and dataloader
model = ...
dataloader = ...

# Run individual tests
gate_stats = analyze_gate_statistics_v2(model, dataloader)
ablation = test_vision_dependency(model, dataloader)
features = check_vision_feature_quality(model, dataloader)
per_type = analyze_per_type_performance(model, dataloader)
```

### Custom analysis

```python
# Example: Track gate evolution during training
gates_per_epoch = []

for epoch in range(num_epochs):
    train(...)
    
    gate_stats, gate_values = analyze_gate_statistics_v2(model, val_loader)
    gates_per_epoch.append(gate_stats['mean'])
    
    print(f"Epoch {epoch}: Mean gate = {gate_stats['mean']:.3f}")

# Plot gate evolution
import matplotlib.pyplot as plt
plt.plot(gates_per_epoch)
plt.xlabel('Epoch')
plt.ylabel('Mean Gate Value')
plt.title('Gate Evolution During Training')
plt.savefig('gate_evolution.png')
```

---

## ğŸš¨ Common Issues & Solutions

### Issue 1: Gate mean < 0.3
**Root cause:** Gating initialized too low or learned to suppress

**Solution:**
```python
model = DeterministicVQA(
    use_vision_gate=True,
    vision_gate_init=3.0,  # ğŸ”¥ Increase from 1.5
    ...
)
```

Or add gate regularization:
```python
# In training loop
gate_penalty = -torch.log(gate_values.mean() + 1e-8)
loss = answer_loss + 0.1 * gate_penalty
```

### Issue 2: Vision drop < 10% but features good
**Root cause:** Dataset bias (text cues too strong)

**Solution:**
1. Filter text-biased questions
2. Add adversarial augmentation
3. Use contrastive VQA (positive/negative image pairs)

### Issue 3: Feature std < 0.1
**Root cause:** DINOv2 frozen and not adapted to domain

**Solution:**
```python
model = DeterministicVQA(
    use_vision_lora=True,  # ğŸ”¥ Enable LoRA fine-tuning
    vision_lora_r=8,
    vision_lora_alpha=16,
    ...
)
```

Or switch encoder:
```python
model = DeterministicVQA(
    dinov2_model_name='openai/clip-vit-large-patch14',  # ğŸ”¥ Use CLIP
    ...
)
```

---

## ğŸ“– References

**Gating mechanisms:**
- Flamingo paper: https://arxiv.org/abs/2204.14198
- Vision-language fusion: https://arxiv.org/abs/2301.12597

**Vision encoders:**
- DINOv2: https://arxiv.org/abs/2304.07193
- CLIP: https://arxiv.org/abs/2103.00020
- SigLIP: https://arxiv.org/abs/2303.15343

**Ablation studies:**
- Vision ablation in VQA: https://arxiv.org/abs/1606.00061
- Modality dropout: https://arxiv.org/abs/1911.12782

---

## âœ… Checklist TrÆ°á»›c Khi Äá»•i Model

- [ ] ÄÃ£ cháº¡y `diagnostic_tools.py`
- [ ] ÄÃ£ xem gate statistics (Test A)
- [ ] ÄÃ£ test vision dependency (Test B)
- [ ] ÄÃ£ check feature quality (Test C)
- [ ] ÄÃ£ phÃ¢n tÃ­ch per-type performance (Test D)
- [ ] ÄÃ£ thá»­ fix gating trÆ°á»›c (náº¿u gate < 0.3)
- [ ] ÄÃ£ thá»­ fix data bias (náº¿u drop < 10% + features good)
- [ ] CÃ³ evidence rÃµ rÃ ng cho viá»‡c Ä‘á»•i encoder

**Chá»‰ Ä‘á»•i encoder KHI:**
- âœ… Drop < 10% (khÃ´ng dÃ¹ng vision)
- âœ… Feature quality BAD (std < 0.1 hoáº·c diversity < 1.0)
- âœ… ÄÃ£ thá»­ fix gate vÃ  data nhÆ°ng khÃ´ng improve

---

**ğŸ’¡ Remember:** 

> "Premature optimization is the root of all evil" - Donald Knuth

> "Evidence over intuition" - Engineering principle

Cháº¡y tests â†’ CÃ³ data â†’ Make decision ğŸ¯
